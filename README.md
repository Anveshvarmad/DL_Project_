README.txt
==========

Project Title: Adversarial Robustness Evaluation on Image Classification Models

Overview:
---------
This project evaluates how adversarial attacks affect the performance of image classification models such as ResNet-34 and DenseNet-121. It includes adversarial image generation, visual analysis, performance comparison, and conclusions based on experimental data.

Repository Contents:
--------------------

1. code/
   â””â”€â”€ Project3Notebook.ipynb
       - Complete implementation of:
         â€¢ Loading pretrained models and test datasets.
         â€¢ Applying FGSM, PGD, and Patch-based adversarial attacks.
         â€¢ Measuring accuracy drops post-attack.
         â€¢ Generating plots for visual analysis.
         â€¢ Summary and conclusions at the end.

2. images/
   â”œâ”€â”€ original_sample.png             â†’ Sample of original unaltered image
   â”œâ”€â”€ fgsm_sample.png                â†’ FGSM-perturbed adversarial example
   â”œâ”€â”€ pgd_sample.png                 â†’ PGD-perturbed adversarial example
   â””â”€â”€ patch_attack_sample.png        â†’ Example image with a visible patch-based attack

3. Plots/
   â”œâ”€â”€ 1.png                          â†’ ResNet-34 Accuracy by Attack
   â”œâ”€â”€ 2.png                          â†’ DenseNet-121 Accuracy by Attack
   â””â”€â”€ 3.png                          â†’ Relative Top-1 Accuracy Drop by Attack

Adversarial Test Set:
---------------------
You can access the adversarial images used for evaluation via the following Google Drive link:

ðŸ”— https://drive.google.com/drive/u/0/folders/1RugxPkkJcx47wv3hybuk-pYU712bOwEg

This folder includes:
â€¢ Images generated using FGSM, PGD, and Patch-based attacks.
â€¢ Subfolders organized by:
    - Attack Type (FGSM, PGD, Patch)
    - Original Class Labels

Instructions:
-------------
1. Open the file `code/Project3Notebook.ipynb` in Jupyter Notebook or JupyterLab.
2. Run all cells to:
   - Load models and test data.
   - Generate adversarial images.
   - Compute accuracy and generate visual plots.
3. To compare the impact of each attack, review the images in the `Plots/` directory.
4. Explore `images/` to view the effect of attacks on sample inputs.
5. For a full dataset of adversarial examples, download from the Google Drive link provided above.

Conclusion:
-----------
This study highlights the vulnerabilities of state-of-the-art models when exposed to even small adversarial perturbations. The performance degradation underscores the importance of incorporating adversarial training and robust model design in practical applications.

This repository can serve as a strong foundation for research or practical implementation of adversarial defenses and robustness evaluation strategies.
